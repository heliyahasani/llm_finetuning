{
    "micro_batch_size": 1,
    "model_name": "m1",
    "tokenizer_name": "t1",
    "epochs": 1,
    "lora_r": 1,
    "max_sequence_length": 100,
    "learning_rate": 1e-05,
    "lora_alpha": 1,
    "lora_dropout": 0.0
}