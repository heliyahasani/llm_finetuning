{
    "max_sequence_length": 469,
    "micro_batch_size": 7,
    "epochs": 75,
    "learning_rate": 0.15912,
    "lora_r": 4,
    "lora_alpha": 5,
    "lora_dropout": 0.09,
    "model_name": "meta-llama/Llama-2-7b-chat-hf",
    "tokenizer_name": "meta-llama/Llama-2-7b-chat-hf",
    "gradient_accumulation_steps": 26,
    "logging_steps": 245
}